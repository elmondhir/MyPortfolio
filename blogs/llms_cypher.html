<!DOCTYPE HTML>

<html>
	<head>
		<link rel="icon" href="../images/favicon.ico" type="image/x-icon">
		<title>Customer Analysis - Chaalal Mohamed</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
		<script src="https://cdn.emailjs.com/dist/email.min.js"></script>
	
		<!-- Prism.js for syntax highlighting -->
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
		<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
		
		<!-- and it's easy to individually load additional languages -->
		<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/go.min.js"></script>
		<script>hljs.highlightAll();</script>

	</head>
	<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-S9EZ6CLD0X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-S9EZ6CLD0X');
</script>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="../index.html" class="logo">Home</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<!-- <li><a href="index.html">This is Massively</a></li> -->
							<li><a href="cnn_project.html">licence Plate Detection</a></li>
							<li><a href="churn_project.html">Customer Analysis</a></li>
							<li class="active"><a href="llms_cypher.html">Fine tuning LLMS</a></li>
							<li><a href="vector_dbs.html">Vector Databases</a></li>

							<!-- <li><a href="elements.html">Elements Reference</a></li> -->
						</ul>
						<ul class="icons">
							<li><a href="https://twitter.com/MondhirChaalal" target="_blank" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="https://www.facebook.com/mondhir.ch" target="_blank" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="https://github.com/elmondhir" target="_blank" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="https://www.linkedin.com/in/mohamed-elmondhir-chaalal/" target="_blank" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
						<section class="post">
							<header class="major">
								<!-- <span class="date">April 25, 2017</span> -->
								<h1>Fine Tune Llms To Learn Neo4j's Cypher<br />
								</h1>
								<p>Discover how to fine-tune Large Language Models (LLMs) to proficiently grasp Neo4j's Cypher query language, enabling seamless navigation and comprehension of graph database querying
								</p>
							</header>
							<div class="image main"><img src="../images/Neo4j-logo_color.png" alt="" /></div>
							<p><b>NB:</b> The full code for this tutorial is set up in my <b><a target="_blank" href="https://github.com/elmondhir/Neo4j-LLMS"> Github Repo</a></b>, with setup instruction for Docker as well</p>
							<h3> Loading dependencies:</h3>
							<p>Graph databases, such as Neo4j, offer a powerful way to model and query interconnected data. Cypher, Neo4j's query language, plays a crucial role in navigating and extracting insights from these graphs. However, mastering Cypher requires time and effort. In this blog post, we'll explore how to leverage Large Language Models (LLMs) to fine-tune their understanding of Cypher, enabling seamless navigation and comprehension of graph database querying.</p>
							
							<h3> Introduction to Large Language Models (LLMs)</h3>
							<p>Large Language Models, such as OpenAI's GPT series, have revolutionized natural language processing tasks. These models, trained on vast amounts of text data, excel in understanding and generating human-like text. Recently, researchers and practitioners have begun exploring the capabilities of LLMs in domain-specific tasks, including programming languages and specialized query languages like Cypher.</p>
							
							<h3>Understanding Cypher</h3>
							<p>Cypher is a declarative query language specifically designed for graph databases. Its syntax resembles ASCII art, making it highly expressive and intuitive for querying graph structures. However, mastering Cypher requires understanding its unique syntax, patterns, and querying techniques.</p>
							
							<div class="row gtr-0 gtr-uniform aln-center">
								<div class="col-9">
									<span class="image fit"><img src="../images/cypher.jpg" alt=""></span>
								</div>
							</div>
							<h3>Fine-Tuning with Peft and LoRA</h3>
							<p><b>LoRA (Low-Rank Adaptation)</b> is a parameter-efficient fine-tuning method that introduces trainable rank decompositions to the weight matrices of a pre-trained model. This approach allows for efficient adaptation to specific tasks while minimizing the number of trainable parameters, thereby reducing computational requirements and memory footprint.</p>

							<p><b>PEFT (Parameter-Efficient Fine-Tuning)</b>, on the other hand, is a broader framework that encompasses various parameter-efficient fine-tuning techniques, including LoRA. PEFT aims to fine-tune large pre-trained models with significantly fewer trainable parameters, making the process more efficient and scalable.</p>
							<div class="row gtr-0 gtr-uniform aln-center">
								<div class="col-9">
									<span class="image fit"><img src="../images/lora_diagram.png" alt="LoRa"></span>
								</div>
							</div>

							<h3>Step 1: Data Preparation</h3>
							<p>For this example, we'll use a dataset consisting of 500 examples of natural language descriptions and their corresponding Cypher queries. This dataset was generated using GPT-4 and can be found in my <b><a target="_blank" href="https://github.com/elmondhir/Neo4j-LLMS"> Github Repository</a></b>. The dataset is in CSV format, with each row containing a natural language description and its corresponding Cypher query.</p>
							<h3>Step 2: Model Selection and Setup</h3>
							<p>For this example, we'll use the CodeLLaMA model, a powerful language model trained specifically for code and natural language understanding. CodeLLaMA has shown excellent performance in various code-related tasks, making it a suitable choice for our Cypher query fine-tuning task.</p>
							<pre><code class="hljs shell"># Model names
model_name = "daryl149/llama-2-7b-chat-hf"

# Activate 4-bit precision base model loading (bool)
load_in_4bit = True


# Activate nested quantization for 4-bit base models (double quantization) (bool)
bnb_4bit_use_double_quant = True


# Quantization type (fp4 or nf4) (string)
bnb_4bit_quant_type = "nf4"


# Compute data type for 4-bit base models
bnb_4bit_compute_dtype = torch.bfloat16


# Load the model 


bnb_config = BitsAndBytesConfig(
		load_in_4bit = load_in_4bit,
		bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,
		bnb_4bit_quant_type = bnb_4bit_quant_type,
		bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,
	)


model, tokenizer = load_model_tokenizer(model_name, bnb_config)</code></pre>
							<p>For this example, we'll use the CodeLLaMA model, a powerful language model trained specifically for code and natural language understanding. CodeLLaMA has shown excellent performance in various code-related tasks, making it a suitable choice for our Cypher query fine-tuning task.</p>
							<h3>Step 3: Define LoRA Configuration</h3>
							<pre><code class="hljs shell">
def create_peft_config(r: int, lora_alpha: int, target_modules, lora_dropout: float, bias: str, task_type: str) -> LoraConfig:
	"""
	Create the Parameter Efficient Fine-Tuning configuration.

	Returns:
		LoraConfig: _description_
	"""
	config = LoraConfig(
		r = r,
		lora_alpha = lora_alpha,
		target_modules = target_modules,
		lora_dropout = lora_dropout,
		bias = bias,
		task_type = task_type,
	)

	return config


################################################################################
# QLoRA parameters
################################################################################

# Number of examples to train (int)
number_of_training_examples = 1000


# Number of examples to use to validate (int)
number_of_valid_examples = 200


# Dataset Name (string)
dataset_name = "b-mc2/sql-create-context"


# LoRA attention dimension (int)
lora_r = 16


# Alpha parameter for LoRA scaling (int)
lora_alpha = 64


# Dropout probability for LoRA layers (float)
lora_dropout = 0.1


# Bias (string)
bias = "none"


# Task type (string)
task_type = "CAUSAL_LM"
</code></pre>
						<h3>STEP 4: Setup the model for training</h3>
						<pre><code class="hljs shell">def preprare_model_for_fine_tune(model: AutoModelForCausalLM,
				lora_r: int,
				lora_alpha: int,
				lora_dropout: float,
				bias: str,
				task_type: str) -> AutoModelForCausalLM:


	# Enable gradient checkpointing to reduce memory usage during fine-tuning
	model.gradient_checkpointing_enable()


	# Prepare the model for training
	model = prepare_model_for_kbit_training(model)


	# Get LoRA module names
	target_modules = find_all_linear_names(model)


	# Create PEFT configuration for these modules and wrap the model to PEFT
	peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)
	model = get_peft_model(model, peft_config)

	model.config.use_cache = False

	return model

model = preprare_model_for_fine_tune(model, lora_r, lora_alpha, lora_dropout, bias, task_type)</code></pre>
						<h3>STEP 5: Finetune the model</h3>
						<pre><code class="hljs shell">def fine_tune(model: AutoModelForCausalLM, trainer: Trainer, output_dir: str) -> None:
	"""
	Fine-tune the model.

	Args:
		model (AutoModelForCausalLM): The model to fine-tune.
		trainer (Trainer): The trainer with the training configuration.
		output_dir (str): The output directory to save the model.
	"""

	print("Training...")

	train_result = trainer.train()

	save_metrics(train_result, trainer)
	save_model(trainer.model, output_dir)

# Training parameters
trainer = Trainer(
	model = model,
	train_dataset = preprocessed_dataset,
	args = TrainingArguments(
		per_device_train_batch_size = per_device_train_batch_size,
		gradient_accumulation_steps = gradient_accumulation_steps,
		warmup_steps = warmup_steps,
		learning_rate = learning_rate,
		fp16 = fp16,
		logging_steps = logging_steps,
		output_dir = output_dir,
		optim = optim,
		num_train_epochs=num_train_epochs
	),
	data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False))

### Start training 
fine_tune(model, trainer, output_dir)	
						</code></pre>
						
						<h3>Step 6: Fine-Tuning Iterations and Refinement</h3>
						<p>Depending on the performance of the fine-tuned model, you may need to iterate and refine the fine-tuning process. This could involve adjusting the LoRA configuration, trying different pre-trained LLMs, or augmenting the training dataset with additional examples.</p>
						<h3>Conculsion</h3>
						<p>Fine-tuning Large Language Models (LLMs) using techniques like LoRA and PEFT opens up new possibilities for enhancing our querying capabilities with graph databases like Neo4j. By training LLMs on Cypher query datasets, we can unlock a range of powerful features, including improved query comprehension, query generation, explanation, and optimization.
							As the field of natural language processing continues to evolve, we can expect even more advanced techniques and applications for fine-tuning LLMs to specific domains and tasks. </p>	
					</section>
					</div>

				<!-- Footer -->
					<footer id="footer">
						
						<section>
							<h2> Contact me</h2>	
							<form id="myForm" method="post" onsubmit="sendEmail(); return false;">
							<div class="fields">
								<div class="field">
									<label for="from_name">Name</label>
									<input type="text" name="from_name" id="from_name" />
								</div>
								<div class="field">
									<label for="email_id">Email</label>
									<input type="text" name="email_id" id="email_id" />
								</div>
								<div class="field">
									<label for="message">Message</label>
									<textarea name="message" id="message" rows="3"></textarea>
								</div>
							</div>
							<ul class="actions">
								<li><input type="submit" value="Send Message" /></li>
							</ul>
						</form>
					</section>
						<section class="split contact">
							<section class="alt">
								<h3>Address</h3>
								<p>32, Rue de Strasbourg<br />
									Saint-Denis, 93200, France</p>
							</section>
							<section>
								<h3>Phone</h3>
								<p> +33787382548</p>
								<!-- <p><a href="#">(000) 000-0000</a></p> -->
							</section>
							<section>
								<h3>Email</h3>
								<p>mondhirch@gmail.com</p>
								<!-- <p><a href="#">info@untitled.tld</a></p> -->
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://twitter.com/MondhirChaalal" target="_blank" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="https://www.facebook.com/mondhir.ch" target="_blank" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
									<li><a href="https://github.com/elmondhir" target="_blank" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
									<li><a href="https://www.linkedin.com/in/mohamed-elmondhir-chaalal/" target="_blank" class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<!-- <div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div> -->

			</div>

			<script src="../assets/js/email.js"></script>
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/jquery.scrollex.min.js"></script>
			<script src="../assets/js/jquery.scrolly.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

			<script src='https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js'></script>
			<script src='https://threejs.org/examples/js/libs/stats.min.js'></script>

			<script  src="../assets/js/particles.js"></script>

	</body>
</html>