<!DOCTYPE HTML>

<html>
	<head>
		<link rel="icon" href="../images/favicon.ico" type="image/x-icon">
		<title>Satellite Imgae Classification </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
		<script src="https://cdn.emailjs.com/dist/email.min.js"></script>
	
		<!-- Prism.js for syntax highlighting -->
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
		<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
		
		<!-- and it's easy to individually load additional languages -->
		<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/go.min.js"></script>
		<script>hljs.highlightAll();</script>

	</head>
	<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-S9EZ6CLD0X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-S9EZ6CLD0X');
</script>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="../index.html" class="logo">Home</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<!-- <li><a href="index.html">This is Massively</a></li> -->
							<li><a href="cnn_project.html">licence Plate Detection</a></li>
							<li ><a href="churn_project.html">Customer Analysis</a></li>
							<li><a href="llms_cypher.html">Fine tuning LLMS</a></li>
							<li><a href="vector_dbs.html">Vector Databases</a></li>
							<li class="active"><a href="sat_image_classification.html">Satellite image classification</a></li>


						</ul>
						<ul class="icons">
							<li><a href="https://twitter.com/MondhirChaalal" target="_blank" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="https://www.facebook.com/mondhir.ch" target="_blank" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="https://github.com/elmondhir" target="_blank" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="https://www.linkedin.com/in/mohamed-elmondhir-chaalal/" target="_blank" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<!-- <span class="date">April 25, 2017</span> -->
									<h1>Satellite Imgae Classification<br />
									</h1>
									<p>Ever wondered how artificial intelligence deciphers Earth's landscapes from above?<br />
										Explore how Satellite Image Classification with Deep Learning helps identify different land cover from remote sensing data<br />
									</p>
								</header>
								<div class="image main"><img src="../images/sat2.jpg" alt="" /></div>
								<p>Recent advances in deep learning have enabled significant progress in computer vision tasks like image classification. One particularly interesting application is classifying satellite imagery, which has many real-world use cases from urban planning to environmental monitoring. In this post, I'll walk through my experience training and evaluating several popular convolutional neural network (CNN) architectures on the RSI-CB256 satellite image dataset.</p>
								<h3> The Dataset</h3>
								<p>The RSI-CB256 dataset contains 36,288 satellite images covering four different classes:</p>
								<ul>
									<li>Barren land</li>
									<li>Vegetation</li>
									<li>Water</li>
									<li>Construction</li>
								</ul>		
								<p>The images were sourced from remote sensing imagery as well as Google Maps snapshots, with each class containing a mix from both sources. At 256x256 pixels, the images are fairly high resolution, allowing the models to pick up on detailed textures and patterns.</p>
								<h3>Data Preparation</h3>
								
								<p>The RSI-CB256 dataset comes pre-organized with the images separated into four folders corresponding to the four classes: barren land, vegetation, water, and construction. This made it easy to use Keras' ImageDataGenerator to efficiently load images during training and evaluation.</p>
								<p>I split the full dataset into 80% train, 10% validation, and 10% test sets using scikit-learn's train_test_split function. Then I created data generators for each split that could load images in batches, apply data augmentation, and preprocess the images on the fly:</p>
								
								<!-- Code Box with Syntax Highlighting -->
								<pre><code class="hljs shell">
def create_gens(train_df, valid_df, test_df, batch_size):
	img_size = (224, 224)
	channels = 3
	img_shape = (img_size[0], img_size[1], channels)
	ts_length = len(test_df)
	def scalar(img):
		return img
	tr_gen = ImageDataGenerator(preprocessing_function= scalar, horizontal_flip= True)

	ts_gen = ImageDataGenerator(preprocessing_function= scalar)
	train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',
										color_mode= 'rgb', shuffle= True, batch_size= 40)
	valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',
										color_mode= 'rgb', shuffle= True, batch_size= 40)
	test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',
										color_mode= 'rgb', shuffle= False, batch_size= 40)
	return train_gen, valid_gen, test_gen

dir = '/kaggle/input/satellite-image-classification/data'

df = create_df(dir)
train_df, test_valid_df = train_test_split(df, test_size=0.2, random_state=42)
test_df, valid_df = train_test_split(test_valid_df, test_size=0.5, random_state=42)

# Get Generators
batch_size = 200
train_gen, valid_gen, test_gen = create_gens(train_df, valid_df, test_df, batch_size)
							</code></pre>
							<p>The key pieces in this code are:</p>

							<ul>
								<li>Defining an <b>ImageDataGenerator</b> for the training set that applies horizontal flips for basic data augmentation.</li>
								<li>Using <b>flow_from_dataframe</b> to generate batches, specifying the directory where the images are located.</li>
								<li>Setting parameters like <b>target image size, color mode, shuffle, batch size, </b>etc.
									Since the dataset was already split into separate class folders, the <b>flow_from_dataframe</b> method automatically loaded the images and associated labels correctly.
								</li>
							</ul>
							<p>I resized the images to 224x224 pixels to match the default input size of the models I tested. Basic augmentation like horizontal flips helps prevent overfitting, though exploring more advanced augmentation could likely boost performance.</p>
							<h3>Model Architectures</h3>
							<p>I decided to test three well-known CNN model families that have shown strong performance on image classification tasks:</p>
							
							<ul>
								<li> <b>EfficientNet:</b> Developed by Google Brain, this model scales up existing architectures like MobileNets and achieves better accuracy and efficiency through compound scaling of dimensions like depth, width and resolution. I used the EfficientNetB0 variant.</li>
								<li> <b>ResNet:</b> The residual network architecture from Microsoft introduced skip connections to allow easier training of very deep networks (up to 152 layers in ResNet-152). I used the ResNet50 variant which is 50 layers deep.</li>
								<li> <b>VGG:</b> The VGG network from Oxford prioritizes depth over width, with very small 3x3 convolutional filters stacked to achieve a large effective receptive field. I used the VGG16 variant with 16 weight layers.</li>
							</ul>
							<p>For each model, I used transfer learning by initializing the weights from an ImageNet pretrained model, then fine-tuning the full network on the RSI-CB256 dataset. <br>
								Here's an example of how I defined and compiled the EfficientNet model:</p>

								<pre><code class="hljs shell">
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint

# Assuming train_gen and valid_gen are your image data generators

img_size = (224, 224)
channels = 3
img_shape = (img_size[0], img_size[1], channels)
class_count = len(list(train_gen.class_indices.keys()))

# Create pre-trained model
base_model = tf.keras.applications.EfficientNetB3(include_top=False, weights="imagenet", input_shape=img_shape, pooling='max')
for layer in base_model.layers:
    layer.trainable = False
model = Sequential([
    base_model,
    BatchNormalization(),
    Dense(256, activation='relu'),
    Dropout(0.3),
     Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(class_count, activation='softmax')
])

optimizer = tf.keras.optimizers.Adam()
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Define filepath to save the best model
filepath = 'best_model.h5'

# Create ModelCheckpoint callback to save the best model based on validation accuracy
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
# Train the model with the added callback
history = model.fit(
    x=train_gen,
    epochs=10,
    verbose=1,
    validation_data=valid_gen,
    callbacks=[checkpoint]
)
								</code></pre>

					<h3>Training Details</h3>
					<p>I trained each model for 50 epochs with a batch size of 32 on Google Colab using freely available GPU resources. I used the RMSprop optimizer with a learning rate of 1e-4. <br>
						The training accuracy plots showed that all three models were able to achieve over 95% accuracy on the training set by the end of training. However, their performance differed on the held-out validation set:
					</p>
					<h3>Results Analysis</h3>
					<p>To evaluate the performance of the models on the test set, we can examine the confusion matrix which shows how often each class was correctly predicted versus misclassified.</p>
						</section>
								
					</div>

					<h3>Conclusions</h3>
					<p>Using transfer learning and relatively modest computational resources, I was able to train models that achieved over 90% accuracy classifying satellite images from the RSI-CB256 dataset into four distinct classes. The results highlight both the power of deep learning for CV as well as the nuanced challenge of parsing high-resolution satellite imagery.
					</p>
				<!-- Footer -->
					<footer id="footer">
						
						<section>
							<h2> Contact me</h2>	
							<form id="myForm" method="post" onsubmit="sendEmail(); return false;">
							<div class="fields">
								<div class="field">
									<label for="from_name">Name</label>
									<input type="text" name="from_name" id="from_name" />
								</div>
								<div class="field">
									<label for="email_id">Email</label>
									<input type="text" name="email_id" id="email_id" />
								</div>
								<div class="field">
									<label for="message">Message</label>
									<textarea name="message" id="message" rows="3"></textarea>
								</div>
							</div>
							<ul class="actions">
								<li><input type="submit" value="Send Message" /></li>
							</ul>
						</form>
					</section>
						<section class="split contact">
							<section class="alt">
								<h3>Address</h3>
								<p>32, Rue de Strasbourg<br />
									Saint-Denis, 93200, France</p>
							</section>
							<section>
								<h3>Phone</h3>
								<p> +33787382548</p>
								<!-- <p><a href="#">(000) 000-0000</a></p> -->
							</section>
							<section>
								<h3>Email</h3>
								<p>mondhirch@gmail.com</p>
								<!-- <p><a href="#">info@untitled.tld</a></p> -->
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://twitter.com/MondhirChaalal" target="_blank" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="https://www.facebook.com/mondhir.ch" target="_blank" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
									<li><a href="https://github.com/elmondhir" target="_blank" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
									<li><a href="https://www.linkedin.com/in/mohamed-elmondhir-chaalal/" target="_blank" class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<!-- <div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div> -->

			</div>

			<script src="../assets/js/email.js"></script>
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/jquery.scrollex.min.js"></script>
			<script src="../assets/js/jquery.scrolly.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

			<script src='https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js'></script>
			<script src='https://threejs.org/examples/js/libs/stats.min.js'></script>

			<script  src="../assets/js/particles.js"></script>

	</body>
</html>